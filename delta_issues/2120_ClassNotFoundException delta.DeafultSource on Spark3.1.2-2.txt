## Issue Description
Hello, I'm trying to use `delta` format on Spark3.1.2-2 w/ scala.
I followed the guide [QuickStart](https://docs.delta.io/latest/quick-start.html) and found compatible delta version w/ [this page](https://docs.delta.io/latest/releases.html)
I used [this maven repo](https://mvnrepository.com/artifact/io.delta/delta-core_2.12) and used Delta version `1.0.1` for spark 3.1.2-2

I built w/ delta dependencies and added configuration during spark submit

### Command & Configs
```bash
spark-submit 
...
    --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
    --conf spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog \
```
and in the source
```scala
    val spark = SparkSession.builder
                      .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                      .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                      .getOrCreate
...

    df.coalesce(16).write.format("delta").save(outputPath)
or
    df.coalesce(16).write.delta(outputPath)
```
https://rmoff.net/2023/04/05/using-delta-from-pyspark-java.lang.classnotfoundexception-delta.defaultsource/
I checked my SparkSession contained all the delta configs 

### Logs

But i got an error w/ such log
```
Exception in thread "main" java.lang.ClassNotFoundException: Failed to find data source: delta. Please find packages at http://spark.apache.org/third-party-projects.html
        at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:692)
        at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:746)
        at org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:993)
        at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:311)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)
        at com.naver.airspace.recsysops.App$.refine(App.scala:66)
        at com.naver.airspace.recsysops.App$.main(App.scala:107)
        at com.naver.airspace.recsysops.App.main(App.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:951)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: delta.DefaultSource
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:666)
        at scala.util.Try$.apply(Try.scala:213)
        at org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:666)
        at scala.util.Failure.orElse(Try.scala:224)
        at org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:666)
        ... 19 more

```


### Related Issue
I checked the issues on this repo, and most of them are using pyspark not the same case as me. https://github.com/delta-io/delta/issues/1013

Besides, I have all the class files on my Ãœber jar (https://github.com/delta-io/delta/issues/700, https://github.com/delta-io/delta/issues/224)
Not only `META-INF/services/org.apache.spark.sql.sources.DataSourceRegister`, but also all the io.delta classes
(I used [`assembly` plugin](https://maven.apache.org/plugins/maven-assembly-plugin/usage.html), therefore i've never been through dependency problem so far)
```
  4705 io/delta/
  4706 io/delta/exceptions/
  4707 io/delta/implicits/
  4708 io/delta/sql/
  4709 io/delta/sql/parser/
  4710 io/delta/storage/
  4711 io/delta/tables/
  4712 io/delta/tables/execution/
...
 46738 org/apache/spark/sql/execution/streaming/OffsetHolder$.class
 46739 org/apache/spark/sql/execution/ui/SparkListenerDriverAccumUpdates.class
 46740 org/apache/spark/sql/execution/ui/SparkListenerSQLAdaptiveSQLMetricUpdates.class
 46741 META-INF/services/org.apache.spark.sql.sources.DataSourceRegister
 46742 org/apache/spark/sql/jdbc/H2Dialect$.class
 46743 org/apache/spark/sql/jdbc/DB2Dialect$.class
 46744 org/apache/spark/sql/api/python/PythonSQLUtils.class
...
```
It seems it contains `DataSourceRegister` but, still can't find the source (https://github.com/delta-io/delta/issues/947)

Could you help me out on this issue ? What am i missing ?

