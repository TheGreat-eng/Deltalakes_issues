### Intro
This PR introduces a new optional configuration _**spark.databricks.delta.properties.defaults.postCommitHookClass**_ that specifies the class name of a custom PostCommitHook. 
This class must extend _**org.apache.spark.sql.delta.hooks.PostCommitHook**_. This feature allows users to execute their own logic after each transaction commit.

For example, 
There is no direct way, if a user wants to register delta table partitions to glue catalog, so it can be further used in Athena or Presto. 
Using this hook implementation, he/she can extract partitions from committedActions and via _alter table add partition_ statement it can be registered to catalog.

### Usage
The user creates their custom hooks by extending PostCommitHook:

```
package org.apache.spark.sql.delta

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.delta.actions.Action
import org.apache.spark.sql.delta.hooks.PostCommitHook

class ExternalPostCommitHook extends PostCommitHook {
  /** A user friendly name for the hook for error reporting purposes. */
  override val name: String = "ExternalPostCommitHook"

  /** Executes the hook. */
  override def run(
    spark: SparkSession,
    txn: OptimisticTransactionImpl,
    committedActions: Seq[Action]): Unit = {

    // Create a Delta table and call the scala api for generating manifest files
    spark.sql(s"GENERATE symlink_ForMat_Manifest FOR TABLE delta.`${txn.deltaLog.dataPath}`")
  }
}
```

They can then use their **_ExternalPostCommitHook_** in their application by setting the config:

`spark.conf.set("spark.databricks.delta.properties.defaults.postCommitHookClass", "org.apache.spark.sql.delta.ExternalPostCommitHook")`

### Testing
This PR includes a unit test that generates the manifest file using the hook.