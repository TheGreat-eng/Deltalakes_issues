Hi everyone, I am creating a delta lake with 6 million rows using uploaded file. And when I tried to merge new uploaded file with 100 rows into this delta lake, I am getting heap out of memory error and spark job is getting aborted. Can someone give any idea how can I resolve this issue? 

I am using EMR and saving delta lake on S3. Instance is 64gb ram, 128 gb hard disk and instance type is M1.xlarge 

emr-5.30.1
Spark 2.4.5
Delta 0.6.1

P.S. I think delta lake is built with scalability in mind but then why am I getting heap out of memory issue when trying to merge large files in existing table.

Any help is appreciated :pray:

Error that I am getting:-

`
 YARN Diagnostics: 
 20/12/08 13:16:25 WARN BlockManager: Persisting block broadcast_41 to disk instead.
 20/12/08 13:16:32 WARN Utils: Suppressing exception in finally: Java heap space
 java.lang.OutOfMemoryError: Java heap space
     at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
     at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
     at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)
     at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$3.apply(TorrentBroadcast.scala:286)
     at org.apache.spark.util.io.ChunkedByteBufferOutputStream.allocateNewChunkIfNeeded(ChunkedByteBufferOutputStream.scala:87)
     at org.apache.spark.util.io.ChunkedByteBufferOutputStream.write(ChunkedByteBufferOutputStream.scala:75)
     at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)
     at net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)
     at com.esotericsoftware.kryo.io.Output.flush(Output.java:185)
     at com.esotericsoftware.kryo.io.Output.close(Output.java:196)
     at org.apache.spark.serializer.KryoSerializationStream.close(KryoSerializer.scala:260)
     at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$blockifyObject$1.apply$mcV$sp(TorrentBroadcast.scala:293)
     at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1414)
     at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:292)
     at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127)
     at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88)
     at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
     at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
     at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1489)
     at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.org$apache$spark$sql$execution$exchange$BroadcastExchangeExec$$doComputeRelation(BroadcastExchangeExec.scala:104)
     at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:63)
     at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anon$1.doCompute(BroadcastExchangeExec.scala:59)
     at org.apache.spark.sql.execution.AsyncDriverOperation$$anonfun$org$apache$spark$sql$execution$AsyncDriverOperation$$compute$1.apply(AsyncDriverOperation.scala:75)
     at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)
     at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)
     at org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:171)
     at org.apache.spark.sql.execution.SQLExecution$$anonfun$withExecutionId$1.apply(SQLExecution.scala:153)
     at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)
     at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:150)
     at org.apache.spark.sql.execution.AsyncDriverOperation.org$apache$spark$sql$execution$AsyncDriverOperation$$compute(AsyncDriverOperation.scala:69)
     at org.apache.spark.sql.execution.AsyncDriverOperation$$anonfun$computeFuture$1.apply$mcV$sp(AsyncDriverOperation.scala:51)
     at org.apache.spark.sql.execution.SQLExecution$$anon$1.run(SQLExecution.scala:221)
 20/12/08 13:16:32 INFO DAGScheduler: Asked to cancel job group e875d44e-4e8a-447a-96fa-9994abb284c7`