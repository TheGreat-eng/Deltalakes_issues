## Feature request

#### Which Delta project/connector is this regarding?
<!--
Please add the component selected below to the beginning of the issue title
For example: [Feature Request][Spark] Title of my issue
-->

- [x] Spark
- [ ] Standalone
- [ ] Flink
- [ ] Kernel
- [ ] Other (fill in here)

### Overview

<!-- Provide a high-level description of the feature request. -->
When writing to Delta with OSS implementation, there is no SQL execution that gets generated that includes an output node with metrics. This differs from Databricks which does include the output node in the SQL execution.

### Motivation

<!-- How will this feature be used? Why is it important? Which users will benefit from it? -->
Capturing output metrics such as size and number of files is incredibly beneficial for monitoring and debugging purposes. Also this is yet another feature disparity with the Databricks Delta implementation.

### Further details

<!--
Use this section to include any additional information about the feature. If you have a proposal for how to implement this feature, please include it here. For implementation guidelines, please read our contributor guidelines: https://github.com/delta-io/delta/blob/master/CONTRIBUTING.md
If there are any specific requirements for this feature that are not immediately obvious please outline them here.
-->

When executing a simple query:

```python
spark.range(10).write.format("delta").mode("append").save("test")
```

OSS generates a SaveIntoDataSourceCommand which is expected with how Delta still uses V1 write:
```
== Physical Plan ==
Execute SaveIntoDataSourceCommand (1)
   +- SaveIntoDataSourceCommand (2)
         +- Range (3)


(1) Execute SaveIntoDataSourceCommand
Output: []

(2) SaveIntoDataSourceCommand
Arguments: org.apache.spark.sql.delta.sources.DeltaDataSource@150dd080, [path=test], Append

(3) Range
Arguments: 0, 10, 1, 10, [id#0L], false
```

And then there's an inner execution plan of everything leading up to the write:
```
== Physical Plan ==
* Range (1)


(1) Range [codegen id : 1]
Output [1]: [id#0L]
Arguments: Range (0, 10, step=1, splits=Some(10))
```

But does not include any node with output metrics.

This differs from Databricks, which has the same parent SaveIntoDataSourceCommand
```
== Physical Plan ==
Execute SaveIntoDataSourceCommand (1)
   +- SaveIntoDataSourceCommand (2)
         +- Range (3)


(1) Execute SaveIntoDataSourceCommand
Output: []

(2) SaveIntoDataSourceCommand
Arguments: com.databricks.sql.transaction.tahoe.sources.DeltaDataSource@35d63d99, [path=/test], Append

(3) Range
Arguments: 0, 10, 1, 8, [id#1177L], false
```

But then a different inner SQL execution:
```
== Physical Plan ==
Execute WriteIntoDeltaCommand (3)
+- WriteFiles (2)
   +- * Range (1)


(1) Range [codegen id : 1]
Output [1]: [id#1177L]
Arguments: Range (0, 10, step=1, splits=Some(8))

(2) WriteFiles
Input [1]: [id#1177L]

(3) Execute WriteIntoDeltaCommand
Input: []
Arguments: OutputSpec(dbfs:/test,Map(),List(id#1177L))
```
 
In Databricks, `WriteIntoDeltaCommand` is a unary node that includes output metrics from data written to the table. In OSS, the inner execution is just generated by the plan going into `TransactionalWrite.writeFiles`: https://github.com/delta-io/delta/blob/685379c8bda4c98b1cd9e5d1c73eb2fe155a6d1e/spark/src/main/scala/org/apache/spark/sql/delta/files/TransactionalWrite.scala#L428

### Willingness to contribute

The Delta Lake Community encourages new feature contributions. Would you or another member of your organization be willing to contribute an implementation of this feature?

- [ ] Yes. I can contribute this feature independently.
- [x] Yes. I would be willing to contribute this feature with guidance from the Delta Lake community.
- [ ] No. I cannot contribute this feature at this time.