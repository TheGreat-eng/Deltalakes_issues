Hi,

We have an azure storage account mounted on our databricks workspace. Using databricks, we upsert delta tables located in the azure storage account. We mounted the azure storage account using a service principal (ABFS driver).

Everything was working fine, until we wanted to switch the mount to a mount using a SAS (WASB driver)
We replaced the mount, now we can not update some of our delta tables.

SAS has the right permissions, also ACL on storage account is fine. Creating a new delta table works with the new mount. Only updating a delta table sometimes gives an error.

We get the following error message:

**22/02/09 16:43:14 INFO NativeAzureFileSystem: FS_OP_CREATE FILE[/visit_occurrence/_delta_log/_last_checkpoint] Creating output stream; permission: rw-r--r--, overwrite: true, bufferSize: 65536
22/02/09 16:43:14 WARN AzureNativeFileSystemStore: WASB request PUT [https://storageaccountname.blob.core.windows.net/unified/visit_occurrence/_delta_log/_last_checkpoint?sig=REDACTED_AZURE_SAS_SIGNATURE&comp=blocklist&api-version=2019-02-02&st=2022-02-07T16%3A04%3A30Z&se=2024-02-07T16%3A04%3A30Z&sv=2018-11-09&spr=https&sp=racwdl&sr=c] failed; status: 409, msg: Blob operation is not supported., request date: Wed, 09 Feb 2022 16:43:14 GMT, start date: Wed Feb 09 16:43:14 UTC 2022, stop date: Wed Feb 09 16:43:14 UTC 2022, service request id: xx, etag: null, md5: null, target: PRIMARY, headers: {null=[HTTP/1.1 409 Blob operation is not supported.], x-ms-version=[2019-02-02], Server=[Windows-Azure-Blob/1.0 Microsoft-HTTPAPI/2.0], x-ms-error-code=[BlobOperationNotSupported], Content-Length=[226], x-ms-request-id=[b989acd1-b01e-0053-77d4-1d062d000000], Date=[Wed, 09 Feb 2022 16:43:14 GMT], x-ms-client-request-id=[a0e6f023-b918-47d7-bde5-26ac609f2894], Content-Type=[application/xml]}
22/02/09 16:43:14 ERROR AzureNativeFileSystemStore: Failed while attempting upload for key visit_occurrence/_delta_log/_last_checkpoint
java.io.IOException: Blob operation is not supported. Please see the cause for further information.**
	at hadoop_azure_shaded.com.microsoft.azure.storage.core.Utility.initIOException(Utility.java:769)
	at hadoop_azure_shaded.com.microsoft.azure.storage.blob.BlobOutputStreamInternal.close(BlobOutputStreamInternal.java:334)
	at shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.storeEmptyLinkFile(AzureNativeFileSystemStore.java:1712)
	at shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1873)
	at shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1654)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$3(DatabricksFileSystemV2.scala:621)
	at com.databricks.s3a.S3AExceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:66)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$2(DatabricksFileSystemV2.scala:618)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$withUserContextRecorded$2(DatabricksFileSystemV2.scala:1013)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withUserContextRecorded(DatabricksFileSystemV2.scala:986)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.$anonfun$create$1(DatabricksFileSystemV2.scala:617)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:477)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:510)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:510)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:452)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:378)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperationWithResultTags(DatabricksFileSystemV2.scala:510)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:510)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.create(DatabricksFileSystemV2.scala:617)
	at com.databricks.backend.daemon.data.client.DatabricksFileSystem.create(DatabricksFileSystem.scala:128)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at com.databricks.tahoe.store.DefaultLogStore.writeInternal(DefaultLogStore.scala:86)
	at com.databricks.tahoe.store.DefaultLogStore.write(DefaultLogStore.scala:72)
	at com.databricks.tahoe.store.DelegatingLogStore.write(DelegatingLogStore.scala:114)
	at com.databricks.sql.transaction.tahoe.Checkpoints.$anonfun$checkpoint$1(Checkpoints.scala:142)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:115)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:477)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:452)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:378)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:125)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:70)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:86)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:402)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:381)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordOperation(DeltaLog.scala:67)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:97)
	at com.databricks.sql.transaction.tahoe.DeltaLog.recordDeltaOperation(DeltaLog.scala:67)
	at com.databricks.sql.transaction.tahoe.Checkpoints.checkpoint(Checkpoints.scala:136)
	at com.databricks.sql.transaction.tahoe.Checkpoints.checkpoint$(Checkpoints.scala:135)
	at com.databricks.sql.transaction.tahoe.DeltaLog.com$databricks$sql$transaction$tahoe$CheckpointsEdge$$super$checkpoint(DeltaLog.scala:67)
	at com.databricks.sql.transaction.tahoe.CheckpointsEdge.checkpoint(CheckpointsEdge.scala:153)
	at com.databricks.sql.transaction.tahoe.CheckpointsEdge.checkpoint$(CheckpointsEdge.scala:75)
	at com.databricks.sql.transaction.tahoe.DeltaLog.checkpoint(DeltaLog.scala:67)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.postCommit(OptimisticTransaction.scala:613)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.postCommit$(OptimisticTransaction.scala:607)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.postCommit(OptimisticTransaction.scala:97)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.$anonfun$commit$1(OptimisticTransactionImplEdge.scala:328)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:115)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:477)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:452)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:378)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:125)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:70)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:86)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:402)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:381)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:97)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:97)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:97)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commit(OptimisticTransactionImplEdge.scala:251)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImplEdge.commit$(OptimisticTransactionImplEdge.scala:247)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.commit(OptimisticTransaction.scala:97)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$run$2(MergeIntoCommandEdge.scala:189)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$run$2$adapted(MergeIntoCommandEdge.scala:152)
	at com.databricks.sql.transaction.tahoe.DeltaLog.withNewTransaction(DeltaLog.scala:205)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$run$1(MergeIntoCommandEdge.scala:152)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.executeThunk$1(MergeIntoCommandEdge.scala:926)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$recordMergeOperation$7(MergeIntoCommandEdge.scala:941)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:377)
	at com.databricks.backend.daemon.driver.ProgressReporter$.withStatusCode(ProgressReporter.scala:363)
	at com.databricks.spark.util.SparkDatabricksProgressReporter$.withStatusCode(ProgressReporter.scala:34)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode(DeltaProgressReporterEdge.scala:30)
	at com.databricks.sql.transaction.tahoe.util.DeltaProgressReporterEdge.withStatusCode$(DeltaProgressReporterEdge.scala:25)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.withStatusCode(MergeIntoCommandEdge.scala:80)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.$anonfun$recordMergeOperation$6(MergeIntoCommandEdge.scala:941)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:115)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:457)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:477)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:452)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:378)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:369)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:341)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:125)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:70)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:57)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:86)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:402)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:381)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.recordOperation(MergeIntoCommandEdge.scala:80)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:113)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:97)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.recordDeltaOperation(MergeIntoCommandEdge.scala:80)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.recordMergeOperation(MergeIntoCommandEdge.scala:938)
	at com.databricks.sql.transaction.tahoe.commands.MergeIntoCommandEdge.run(MergeIntoCommandEdge.scala:150)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.Dataset.$anonfun$logicalPlan$1(Dataset.scala:233)
	at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3815)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:126)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:269)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:104)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:219)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3813)
	at org.apache.spark.sql.Dataset.<init>(Dataset.scala:233)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:852)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:92)
	at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$2(DeltaMergeBuilder.scala:219)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:279)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:271)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:20)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:61)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:147)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:463)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:462)
	at io.delta.tables.DeltaMergeBuilder.withAttributionTags(DeltaMergeBuilder.scala:124)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordAPILang(DeltaLogging.scala:128)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordAPILang$(DeltaLogging.scala:123)
	at io.delta.tables.DeltaMergeBuilder.recordAPILang(DeltaMergeBuilder.scala:124)
	at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:219)
	at com.databricks.sql.transaction.tahoe.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:90)
	at com.databricks.sql.transaction.tahoe.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:76)
	at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:124)
	at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:216) 


We don't want to switch back to using the service principal.