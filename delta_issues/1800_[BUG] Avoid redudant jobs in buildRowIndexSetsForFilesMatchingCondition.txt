## Bug

#### Which Delta project/connector is this regarding?
<!--
Please add the component selected below to the beginning of the issue title
For example: [BUG][Spark] Title of my issue
-->

- [x] Spark
- [ ] Standalone
- [ ] Flink
- [ ] Kernel
- [ ] Other (fill in here)

### Describe the problem
In `buildRowIndexSetsForFilesMatchingCondition` we join the results DataFrame with the candidate file list to fetch the Deletion Vector Descriptors. Currently, there is a sanity check there to verify we do not eliminate any rows due to path encoding issues. The sanity check compares before and after counts. The counts produce 2 extra jobs which can cause a significant performance overhead. For example, in the case of merge each job could be a full shuffle join between the source and the target. We need to eliminate the extra jobs while maintaining the sanity check.

#### Steps to reproduce
N/A.

<!--
Please include copy-pastable code snippets if possible.
1. _____
2. _____
3. _____
-->

#### Observed results

<!-- What happened?  This could be a description, log output, etc. -->

#### Expected results

<!-- What did you expect to happen? -->

#### Further details

<!--
Include any additional details that may be useful for diagnosing the problem here. If including tracebacks, please include the full traceback. Large logs and files should be attached.
-->

### Environment information

* Delta Lake version:
* Spark version:
* Scala version:

### Willingness to contribute

The Delta Lake Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the Delta Lake code base?

- [x] Yes. I can contribute a fix for this bug independently.
- [ ] Yes. I would be willing to contribute a fix for this bug with guidance from the Delta Lake community.
- [ ] No. I cannot contribute a bug fix at this time.
