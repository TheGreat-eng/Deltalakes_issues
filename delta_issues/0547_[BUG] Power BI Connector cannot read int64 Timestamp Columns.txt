## Bug

### Describe the problem

#### Steps to reproduce

1. Create a Test File (here using Databricks)
```python
spark.conf.set("spark.sql.parquet.outputTimestampType", "TIMESTAMP_MILLIS")
spark.sql("create table test_data using delta location 'abfss://MYPATH/repo/test';")

from datetime import datetime, timezone
spark.createDataFrame([{"date": datetime.now(tz=timezone.utc)}]).write.option("mergeSchema", "true").mode("append").save(MYPATH)
```

The resulting delta file: [repo.zip](https://github.com/user-attachments/files/17393476/repo.zip)

2. Read it using fn_ReadDeltaTable in Power BI:

```m
let
    Source = AzureStorage.DataLake("MYPATH", [HierarchicalNavigation = false]),
    DeltaTable = fn_ReadDeltaTable(Source)
in
    DeltaTable

```



#### Observed results

Column is empty:
![image](https://github.com/user-attachments/assets/8403e709-2ed0-402b-a001-3842c8adb732)


#### Expected results

Column should correctly display the data, as does databricks
![image](https://github.com/user-attachments/assets/dcea41dc-35e6-4fc8-99fc-797a5db2ab76)


### Willingness to contribute

The Delta Lake Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the Delta Lake code base?

- [ ] Yes. I can contribute a fix for this bug independently.
- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the Delta Lake community.
- [ ] No. I cannot contribute a bug fix at this time.

I have to admit I don't really understand the Power Query Code of the Connector :)

