for all other spark native format using following code works:
spark.read.format('csv').option("fs.s3a.access.key", key).option("fs.s3a.secret.key", secret).load("s3a://<bucket/<path to csv>")
but for delta using
spark.read.format('delta').option("fs.s3a.access.key", key).option("fs.s3a.secret.key", secret).load("s3a://<bucket/<path to delta table>")
will throw exception with the s3 credentials provider not found error.
We knew we can set the hadoop level configuration, but our application using a shared spark context, would like to limit the cred scope to the to DataframeReader level, wondering if delta lake can address this issue and make it similar experience as other spark native format.
