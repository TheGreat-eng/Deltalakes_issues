```
val initLimitDF = df.groupBy("personId","dtime_call_model").agg(min("eventTime"))
initLimitDF.show()
+--------+--------------------+--------------------+
|personId|    dtime_call_model|      min(eventTime)|
+--------+--------------------+--------------------+
|1234567|2019-11-14 02:28:...|2019-11-14 02:31:...|


deltaTable.as("old")
          .merge(
            initLimitDF.as("new"),
            "old.id_person = new.personId and old.dtime_call_model = new.dtime_call_model")
          .whenMatched()
          .update(Map("Initial_limit" -> when($"new.code" === 15271, expr("new.loanAmount")))))
          .execute()
```
Exception shows:
```
org.apache.spark.sql.AnalysisException: cannot resolve `new.code` in UPDATE clause given columns `new.loanAmount`, `new.loanAmountCash`, `old.dtime_call_model`, `new.code`, `new.personId`, `new.dtime_call_model`, `old.id_person`;
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.plans.logical.MergeInto$$anonfun$org$apache$spark$sql$catalyst$plans$logical$MergeInto$$resolveOrFail$1$3.apply(merge.scala:248)
	at org.apache.spark.sql.catalyst.plans.logical.MergeInto$$anonfun$org$apache$spark$sql$catalyst$plans$logical$MergeInto$$resolveOrFail$1$3.apply(merge.scala:244)
```

As we can see, it shows "cannot resolve `new.code` in UPDATE clause given columns ........", that make me confused.
The correct description I think is "cannot resolve `new.code` in UPDATE clause given columns 'personId,dtime_call_model,min(eventTime)'"