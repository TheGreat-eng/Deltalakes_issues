## Bug

#### Which Delta project/connector is this regarding?
<!--
Please add the component selected below to the beginning of the issue title
For example: [BUG][Spark] Title of my issue
-->

- [x] Spark
- [ ] Standalone
- [ ] Flink
- [ ] Kernel
- [ ] Other (fill in here)

### Describe the problem
Merging with automatic schema evolution enabled doesn't seem to take custom set expressions into account for determining schema updates.

#### Steps to reproduce

<!--
Please include copy-pastable code snippets if possible.
1. _____
2. _____
3. _____
-->
```python
from delta.tables import DeltaTable

from pyspark.sql import SparkSession

spark = (SparkSession.Builder()
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
    .config("spark.databricks.delta.schema.autoMerge.enabled", "true")
    .getOrCreate()
)

spark.createDataFrame([["a"], ["b"], ["c"]], ["old"]).write.format("delta").mode("overwrite").save("merge-test")

df = spark.createDataFrame([["a", "1"], ["b", "2"], ["c", "3"]], ["first", "second"])

dt = DeltaTable.forPath(spark, "file:/path/to/table/merge-test")
(dt.alias("target").merge(df.alias("source"), "target.old = source.first")
    .whenMatchedUpdate(set = {
        "new": "source.second"
    })   
    .execute()
)
```

#### Observed results

<!-- What happened?  This could be a description, log output, etc. -->
```
pyspark.errors.exceptions.captured.AnalysisException: [DELTA_MERGE_UNRESOLVED_EXPRESSION] Cannot resolve new in UPDATE clause given columns source.first, source.second
```

#### Expected results

<!-- What did you expect to happen? -->
Table would be updated to add a `new` column.

#### Further details

<!--
Include any additional details that may be useful for diagnosing the problem here. If including tracebacks, please include the full traceback. Large logs and files should be attached.
-->
If you use the same name for the column on the source side, it works as expected.

```python
df = spark.createDataFrame([["a", "1"], ["b", "2"], ["c", "3"]], ["first", "new"])

dt = DeltaTable.forPath(spark, "file:/path/to/table/merge-test")
(dt.alias("target").merge(df.alias("source"), "target.old = source.first")
    .whenMatchedUpdate(set = {
        "new": "source.new"
    })   
    .execute()
)
```

The code for merging is fairly complex, but it seems like only the schema of the source table is considered for schema evolution. This makes sense for insert all and update all operations, but doesn't make sense for custom set expressions that could arbitrarily manipulate the source or target DataFrame to compute new values.

### Environment information

* Delta Lake version: 3.0.0
* Spark version: 3.5.0
* Scala version: 2.12

### Willingness to contribute

The Delta Lake Community encourages bug fix contributions. Would you or another member of your organization be willing to contribute a fix for this bug to the Delta Lake code base?

- [ ] Yes. I can contribute a fix for this bug independently.
- [x] Yes. I would be willing to contribute a fix for this bug with guidance from the Delta Lake community.
- [ ] No. I cannot contribute a bug fix at this time.
