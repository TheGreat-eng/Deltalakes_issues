### Describe the problem
I'm running spark thrift server and fetching query from delta table (stored in hive metastore). 

 It works fine for some time (few hours) and then it will start throwing NPE for all delta queries. Any pointers would be greatly appreciated. Thanks in advance.

```

org.apache.hive.service.cli.HiveSQLException: Error running query: java.lang.NullPointerException
            at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:44)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:334)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:233)
            at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
            at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)
            at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:43)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:233)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:228)
            at java.base/java.security.AccessController.doPrivileged(Native Method)
            at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
            at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:242)
            at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
            at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
            at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
            at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
            at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: java.lang.NullPointerException
            at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:359)
            at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:162)
            at org.apache.spark.SparkContext.clean(SparkContext.scala:2501)
            at org.apache.spark.rdd.RDD.$anonfun$map$1(RDD.scala:422)
            at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
            at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
            at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
            at org.apache.spark.rdd.RDD.map(RDD.scala:421)
            at org.apache.spark.sql.delta.util.StateCache$CachedDS.<init>(StateCache.scala:57)
            at org.apache.spark.sql.delta.util.StateCache.$anonfun$cacheDS$1(StateCache.scala:107)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:145)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:144)
            at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:59)
            at org.apache.spark.sql.delta.util.StateCache.cacheDS(StateCache.scala:107)
            at org.apache.spark.sql.delta.util.StateCache.cacheDS$(StateCache.scala:106)
            at org.apache.spark.sql.delta.Snapshot.cacheDS(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.$anonfun$cachedState$1(Snapshot.scala:166)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:145)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:144)
            at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.cachedState$lzycompute(Snapshot.scala:166)
            at org.apache.spark.sql.delta.Snapshot.cachedState(Snapshot.scala:165)
            at org.apache.spark.sql.delta.Snapshot.$anonfun$stateDF$1(Snapshot.scala:176)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:145)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:144)
            at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.stateDF(Snapshot.scala:176)
            at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$3(Snapshot.scala:221)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)
            at org.apache.spark.sql.delta.Snapshot.recordFrameProfile(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$2(Snapshot.scala:217)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:145)
            at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:144)
            at org.apache.spark.sql.delta.Snapshot.withDmqTag(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.$anonfun$computedState$1(Snapshot.scala:217)
            at org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)
            at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)
            at org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)
            at org.apache.spark.sql.delta.Snapshot.withStatusCode(Snapshot.scala:59)
            at org.apache.spark.sql.delta.Snapshot.computedState$lzycompute(Snapshot.scala:216)
            at org.apache.spark.sql.delta.Snapshot.computedState(Snapshot.scala:214)
            at org.apache.spark.sql.delta.Snapshot.metadata(Snapshot.scala:254)
            at org.apache.spark.sql.delta.stats.DataSkippingReaderBase.$init$(DataSkippingReader.scala:170)
            at org.apache.spark.sql.delta.Snapshot.<init>(Snapshot.scala:69)
            at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$createSnapshot$1(SnapshotManagement.scala:298)
            at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment(SnapshotManagement.scala:426)
            at org.apache.spark.sql.delta.SnapshotManagement.createSnapshotFromGivenOrEquivalentLogSegment$(SnapshotManagement.scala:414)
            at org.apache.spark.sql.delta.DeltaLog.createSnapshotFromGivenOrEquivalentLogSegment(DeltaLog.scala:65)
            at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot(SnapshotManagement.scala:288)
            at org.apache.spark.sql.delta.SnapshotManagement.createSnapshot$(SnapshotManagement.scala:283)
            at org.apache.spark.sql.delta.DeltaLog.createSnapshot(DeltaLog.scala:65)
            at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$2(SnapshotManagement.scala:269)
            at scala.Option.map(Option.scala:230)
            at org.apache.spark.sql.delta.SnapshotManagement.$anonfun$getSnapshotAtInit$1(SnapshotManagement.scala:262)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)
            at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:65)
            at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit(SnapshotManagement.scala:260)
            at org.apache.spark.sql.delta.SnapshotManagement.getSnapshotAtInit$(SnapshotManagement.scala:258)
            at org.apache.spark.sql.delta.DeltaLog.getSnapshotAtInit(DeltaLog.scala:65)
            at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:54)
            at org.apache.spark.sql.delta.DeltaLog.<init>(DeltaLog.scala:70)
            at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:595)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
            at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:591)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:141)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:139)
            at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:460)
            at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:134)
            at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)
            at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)
            at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:460)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:133)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:123)
            at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:111)
            at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:460)
            at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:590)
            at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:606)
            at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4876)
            at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3528)
            at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2277)
            at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2154)
            at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2044)
            at com.google.common.cache.LocalCache.get(LocalCache.java:3952)
            at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4871)
            at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:606)
            at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:613)
            at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:511)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:85)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:85)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:114)
            at scala.Option.getOrElse(Option.scala:189)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:114)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:102)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:120)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:118)
            at org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:122)
            at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:175)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1284)
            at scala.Option.map(Option.scala:230)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$lzycompute$1(Analyzer.scala:1257)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.loaded$1(Analyzer.scala:1257)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1294)
            at scala.Option.orElse(Option.scala:447)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1293)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1211)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$15.applyOrElse(Analyzer.scala:1174)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)
            at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
            at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)
            at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1122)
            at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1121)
            at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)
            at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1174)
            at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1140)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
            at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
            at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
            at scala.collection.immutable.List.foldLeft(List.scala:91)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
            at scala.collection.immutable.List.foreach(List.scala:431)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:222)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:218)
            at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:167)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:218)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:182)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
            at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
            at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:203)
            at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)
            at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:202)
            at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:88)
            at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
            at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
            at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
            at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
            at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:88)
            at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:86)
            at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:78)
            at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)
            at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
            at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)
            at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)
            at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
            at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)
            at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)
            at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:300)
            ... 16 more
```

#### Steps to reproduce
1. Store some data in metastore delta table
2. Fetch it from Spark thrift server, using any SQL client. 


### Environment information

* Delta Lake version: 2.0.1 / 1.2.1
* Spark version: 3.2.1
* Scala version: 2.12
