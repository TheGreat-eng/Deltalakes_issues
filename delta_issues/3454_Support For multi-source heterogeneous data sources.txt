We divided data sources into four categories: structured data, semi-structured data, unstructured data and binary data. Separately, structured data includes three relational databases that we use most frequently, which are MySQL, Oracle and SQL Server. Semi-structured data includes NoSQL databases and files in format of csv, xml, json, html, etc. Moreover, we divided NoSQL databases into key-value databases (e.g., Redis), column-oriented databases (e.g., HBase), document databases (e.g., MongoDB) and graph databases (e.g., Neo4j). And unstructured data includes text files, multi-media files such as MP3, MP4 and pictures. Binary data includes the data which transmitted over serial ports and Bluetooth.
To realize the connection between four classification data sources and Delta Lake, we provide the source code for reading and writing relational databases data, HBase data, local files and  remote files using Delta Lake. The relational database mainly refers to the JDBC module of Spark, based on that module, we simplify the usage method by using configuration files. We also supplement the conversion of some field types in MySQL, Oracle and SQL Server; Phoenix is a JDBC driver of HBase, using Phoenix can support users to interact with HBase in a way similar to the relational database; Supports for files mainly include the operation on remote files, local files and file streams.
MULTI_DATASOURCE_GUIDE.md describes how to perform the function mentioned above through Spark-Shell, please read it carefully.
We include all the required dependency in build.sbt. In addition, we add a directory named configuration, which includes configurations of different data sources.
In the near future, we will support the connection between Delta Lake and Redis, MongoDB, Neo4j, Serial Ports, Bluetooth, etc. In addition, storage layer will expands support for Ceph.