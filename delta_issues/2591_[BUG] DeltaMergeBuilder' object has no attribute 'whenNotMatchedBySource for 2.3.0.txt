## Bug

```
DeltaMergeBuilder' object has no attribute 'whenNotMatchedBySource
```

for version 2.3.0 of delta lake and Spark 3.3.2

### Describe the problem

```
import pandas as pd
import pyspark
from pyspark.sql import functions as F
from pyspark.sql import SparkSession
from delta import *

def get_pyspark_config():
    return {
                "spark.master": "local[2]",
                "spark.driver.memory": "2G",
                "spark.sql.extensions":"io.delta.sql.DeltaSparkSessionExtension",
                "spark.sql.catalog.spark_catalog":"org.apache.spark.sql.delta.catalog.DeltaCatalog",
                "spark.jars.packages": "io.delta:delta-core_2.12:2.3.0",
            }
s_config = get_pyspark_config()
builder = SparkSession.builder.appName("delta_testing").master('local[2]')

for key, value in s_config.items():
    builder = builder.config(key, value)

spark = builder.getOrCreate()
spark.sparkContext.setLogLevel('ERROR')  
spark
```

this logs v3.3.2 for spark and found io.delta#delta-core_2.12;2.3.0 in central .e. the expected values

now creating some dummy data
```
d1 = spark.createDataFrame(pd.DataFrame({'key':[1,2,3], 'value':[4,5,6],'value2':["a", "b", "c"], 'date':[1,1,1]}))
d2 = spark.createDataFrame(pd.DataFrame({'key':[1,2], 'value':[4,5], 'date':[2,2],'value2':["a", "b"]}))
```

initial first write of data (first load of data)

```
d1.withColumn("is_current", F.lit(True)).withColumn("valid_to", F.lit(None).astype("int")).write.mode("overwrite").format("delta").save("dummy_delta")
#.option("readChangeFeed", "true")
delta_state = spark.read.format("delta").load("dummy_delta")
delta_state.printSchema()
delta_state.show()
```

fails with: AttributeError: 'DeltaMergeBuilder' object has no attribute 'whenNotMatchedBySource'

```
def handle_merge(path, updates, current_dt):
    delta_state = DeltaTable.forPath(spark, path)
    delta_state.alias("s").merge(updates.alias("c"),
      "s.key = c.key and s.is_current = true") \
    .whenMatchedUpdate(
      condition = "s.is_current = true AND s.value <> c.value",
      set = {
        "is_current": "false",
        "valid_to": "c.date"
      }
    ).whenNotMatchedInsert(
      values = {
        "key": "c.key",
        "value": "c.value",
        "value2": "c.value2",
        "is_current": "true",
        "date": "c.date",
        "valid_to": "null"
      }
    ).whenNotMatchedBySource(condition = "c.valid_to IS NULL").update(set = {
        "is_current": "false",
        "valid_to": current_dt
      }).whenNotMatchedBySource().delete().execute()

    delta_state = spark.read.format("delta").load("dummy_delta")
    delta_state.show(50)

handle_merge('dummy_delta', d2, current_dt=2)
```