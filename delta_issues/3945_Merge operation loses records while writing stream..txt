Hello, i have been working for the past week reading from eventhubs with databricks spark and writing to a delta table, in my use case i must handle duplicated records, and i have been trying to use this example as a guide: https://docs.databricks.com/delta/delta-update.html#merge-examples-1 but i seem to encounter a problem where some records are never inserted, i made a more specific explanation in stack overflow (https://stackoverflow.com/questions/60865007/deduplicating-stream-loses-events-spark-eventhubs-in-databricks) but i don't seem to get any opinions on the matter. Could it be possible that the merge operation for the delta table in python is buggy? it seems to have varying behaviours depending on the stream size, is this as intended? i tried to write it in Scala but the result was the same. 
