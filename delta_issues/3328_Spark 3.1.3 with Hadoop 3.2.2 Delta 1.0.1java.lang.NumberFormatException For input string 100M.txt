I have the current situation:

Delta table located in S3
I want to write this table.
spark version 3.1.3 and hadoop 3.2.2

`        
SparkSession spark = SparkSession.builder()
      .appName("spark_kafka_s3")
      .master("local[2]") 
      .config("spark.jars.packages", "io.delta:delta-core_2.12:1.0.1")
      .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
      .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
      .getOrCreate();
`
I try to read message from kafka and write the analysis results in to S3
`
head.coalesce(1).writeStream()
                .format("delta")
                .option("checkpointLocation",path +  "/checkpoint/" + headTableName)
                .option("mergeSchema", "true")
                .outputMode("append")
                .partitionBy("ds", "hh")
                .start(path + "/" + headTableName);
`
My script is like this
`
bin/spark-submit --master spark://server01:xxx --packages io.delta:delta-core_2.12:1.0.1,com.amazonaws:aws-java-sdk-pom:1.11.1034,org.apache.hadoop:hadoop-aws:3.2.2 --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --class org.example.App /root/x/out/artifacts/x/x.jar 1
`

I got error like this:
![image](https://user-images.githubusercontent.com/30817980/157198504-a6fd1bfe-34d1-4c56-a2e1-15d7c3bb1d95.png)

I have read the issue #639 https://github.com/delta-io/delta/issues/639
However, Delta 1.01 is proposed to support Spark 3.1.x.
So I have no idea how to solve the problem. 
Greatful for anyone potentially encountering the same issue.