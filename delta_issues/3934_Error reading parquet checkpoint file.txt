I have a number of Delta tables that are updated infrequently and when querying them on a cluster that has been running for a while I run into errors like the following:

```com.databricks.sql.io.FileReadException: Error while reading file s3://bucket/subdir/_delta_log/00000000000000000020.checkpoint.parquet. It is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/Dataframe involved.```

What is happening here is that it is still referring to an outdated parquet checkpoint file which has since been removed due to the default 2 day retention rule https://github.com/delta-io/delta/blob/master/src/main/scala/org/apache/spark/sql/delta/DeltaConfig.scala#L264 . I am aware that we could also turn off the cleanup process entirely with the `enableExpiredLogCleanup` setting https://github.com/delta-io/delta/blob/master/src/main/scala/org/apache/spark/sql/delta/DeltaConfig.scala#L281 . But the problem is that even though yes, this checkpoint file no longer exists, the `_last_checkpoint` file in S3 does in fact reference the most recent valid parquet checkpoint file. I would expect the driver to at least check this and updating the reference once before returning an error. The problem is especially bad because in many cases it seems to depend on how long the cluster has been running, not how long the query/job has been running.

Since I can't find this exact error in this repo, I suspect it is being generated by the Spark driver, but wanted to ask your advice first. Firstly, the advice in the error message unfortunately does not help: `REFRESH TABLE` does not have any effect. In other threads I have also seen it suggested to run `FSCK REPAIR`, but this throws the same error as above. 

The two things I have found to work are:
 - restarting the cluster, which removes the DBIO fragments, or
 - calling `UNCACHE TABLE database.tableName`

Even though this is a workaround, it is suboptimal to have to call this before all queries, and I don't know beforehand which tables are updated infrequently enough that this might be a problem. It would be better if the driver could attempt to handle this case transparently before throwing an error.

Do you think this is possible? If so, do you know where can we make the change, and would it be accepted as a PR? 