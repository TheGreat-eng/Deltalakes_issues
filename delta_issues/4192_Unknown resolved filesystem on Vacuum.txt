We’re seeing an odd behavior trying to maintain our Delta Lake tables.  After running the OPTIMIZE, a run of the VACUUM command is throwing an error.  Only 1 web result came back on a search and it wasn’t helpful.  Could you please let me know if there is a remedy for this? 

Tried both these commands:
spark.sql("VACUUM <table name>")
spark.sql("VACUUM '<mount path directory>'")


org.apache.spark.SparkException: Job aborted due to stage failure: Task 23 in stage 611.0 failed 4 times, most recent failure: Lost task 23.3 in stage 611.0 (TID 13019, 10.31.248.7, executor 2): java.lang.IllegalStateException: Unknown resolved filesystem under DatabricksFileSystemV2: class com.databricks.spark.metrics.FileSystemWithMetrics

at com.databricks.tahoe.store.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:163) at com.databricks.tahoe.store.DelegatingLogStore.listFrom(DelegatingLogStore.scala:111) at com.databricks.sql.util.FileOperations$.com$databricks$sql$util$FileOperations$$list$1(FileOperations.scala:139) at com.databricks.sql.util.FileOperations$$anonfun$3.apply(FileOperations.scala:153) at com.databricks.sql.util.FileOperations$$anonfun$3.apply(FileOperations.scala:152) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2098) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2086) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2085) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2085) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:1076) at scala.Option.foreach(Option.scala:257) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1076) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2317) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2265) at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2253) at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:873) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2258) at org.apache.spark.SparkContext.runJob(SparkContext.scala:2356) at org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:245) at org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:280) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:80) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:86) at org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508) at org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480) at org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:312) at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:288) at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2862) at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2861) at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3423) at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3418) at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:92) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:233) at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:86) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:163) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3418) at org.apache.spark.sql.Dataset.count(Dataset.scala:2861) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$$anonfun$gc$1$$anonfun$apply$1.apply(DeltaVacuumCommand.scala:192) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$$anonfun$gc$1$$anonfun$apply$1.apply(DeltaVacuumCommand.scala:121) at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:757) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$$anonfun$gc$1.apply(DeltaVacuumCommand.scala:121) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$$anonfun$gc$1.apply(DeltaVacuumCommand.scala:121) at com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:359) at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230) at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:13) at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268) at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:13) at com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:345) at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:13) at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:36) at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:65) at com.databricks.spark.util.UsageLogger$class.recordOperation(UsageLogger.scala:66) at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:44) at com.databricks.spark.util.UsageLogging$class.recordOperation(UsageLogger.scala:292) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$.recordOperation(DeltaVacuumCommand.scala:41) at com.databricks.sql.transaction.tahoe.metering.DeltaLogging$class.recordDeltaOperation(DeltaLogging.scala:90) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$.recordDeltaOperation(DeltaVacuumCommand.scala:41) at com.databricks.sql.transaction.tahoe.commands.DeltaVacuumCommand$.gc(DeltaVacuumCommand.scala:117) at com.databricks.sql.transaction.directory.VacuumTableCommand$$anonfun$run$1.apply(VacuumTableCommand.scala:58) at com.databricks.sql.transaction.directory.VacuumTableCommand$$anonfun$run$1.apply(VacuumTableCommand.scala:43) at com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:775) at com.databricks.sql.transaction.directory.VacuumTableCommand.run(VacuumTableCommand.scala:43) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:72) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:70) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:81) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205) at org.apache.spark.sql.Dataset$$anonfun$6.apply(Dataset.scala:205) at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3423) at org.apache.spark.sql.Dataset$$anonfun$55.apply(Dataset.scala:3418) at org.apache.spark.sql.execution.SQLExecution$$anonfun$withCustomExecutionEnv$1.apply(SQLExecution.scala:92) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:233) at org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:86) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:163) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3418) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:205) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:89) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:696) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command-4090825574753937:1) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw$$iw$$iw$$iw$$iw.<init>(command-4090825574753937:46) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw$$iw$$iw$$iw.<init>(command-4090825574753937:48) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw$$iw$$iw.<init>(command-4090825574753937:50) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw$$iw.<init>(command-4090825574753937:52) at linee253c5bed24840f1b1d974f060a9a28156.$read$$iw.<init>(command-4090825574753937:54) at linee253c5bed24840f1b1d974f060a9a28156.$read.<init>(command-4090825574753937:56) at linee253c5bed24840f1b1d974f060a9a28156.$read$.<init>(command-4090825574753937:60) at linee253c5bed24840f1b1d974f060a9a28156.$read$.<clinit>(command-4090825574753937) at linee253c5bed24840f1b1d974f060a9a28156.$eval$.$print$lzycompute(<notebook>:7) at linee253c5bed24840f1b1d974f060a9a28156.$eval$.$print(<notebook>:6) at linee253c5bed24840f1b1d974f060a9a28156.$eval.$print(<notebook>) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786) at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047) at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638) at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637) at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31) at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19) at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569) at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565) at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:199) at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:189) at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189) at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:189) at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:534) at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:489) at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:189) at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$5.apply(DriverLocal.scala:273) at com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$5.apply(DriverLocal.scala:253) at com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230) at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:42) at com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:268) at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:42) at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:253) at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589) at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:589) at scala.util.Try$.apply(Try.scala:192) at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:584) at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:475) at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:542) at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:381) at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:328) at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:215) at java.lang.Thread.run(Thread.java:748) Caused by: java.lang.IllegalStateException: Unknown resolved filesystem under DatabricksFileSystemV2: class com.databricks.spark.metrics.FileSystemWithMetrics at com.databricks.tahoe.store.DelegatingLogStore.getDelegate(DelegatingLogStore.scala:163) at com.databricks.tahoe.store.DelegatingLogStore.listFrom(DelegatingLogStore.scala:111) at com.databricks.sql.util.FileOperations$.com$databricks$sql$util$FileOperations$$list$1(FileOperations.scala:139) at com.databricks.sql.util.FileOperations$$anonfun$3.apply(FileOperations.scala:153) at com.databricks.sql.util.FileOperations$$anonfun$3.apply(FileOperations.scala:152) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55) at org.apache.spark.scheduler.Task.doRunTask(Task.scala:139) at org.apache.spark.scheduler.Task.run(Task.scala:112) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$13.apply(Executor.scala:496) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1432) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:502) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)
