We use delta_core_2.12:1.0.0 in our project and use sbt to build an Uber jar (marked delta_core as "provided"). It works fine locally, but throws following exception while running in Databricks Runtime 8.X
```
21/07/20 19:27:43 ERROR ScalaDriverLocal: User Code Stack Trace: 
java.lang.NoClassDefFoundError: org/apache/spark/sql/delta/DeltaOptions
	at hubble.KafkaSilverGeneratorRunner$.run(KafkaSilverGeneratorRunner.scala:24)
	at hubble.KafkaSilverGeneratorRunner$.main(KafkaSilverGeneratorRunner.scala:44)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(command--1:1)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw$$iw$$iw$$iw$$iw.<init>(command--1:43)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw$$iw$$iw$$iw.<init>(command--1:45)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw$$iw$$iw.<init>(command--1:47)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw$$iw.<init>(command--1:49)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$$iw.<init>(command--1:51)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read.<init>(command--1:53)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$.<init>(command--1:57)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$read$.<clinit>(command--1)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$eval$.$print$lzycompute(<notebook>:7)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$eval$.$print(<notebook>:6)
	at $line97ae24d00eb6470c865f64e69fe8fb7225.$eval.$print(<notebook>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:745)
	at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1021)
	at scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:574)
	at scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:41)
	at scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:37)
	at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)
	at scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:573)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:600)
	at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:570)
	at com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:219)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:235)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:852)
	at com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:805)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:235)
	at com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$12(DriverLocal.scala:494)
	at com.databricks.logging.UsageLogging.$anonfun$withAttributionContext$1(UsageLogging.scala:240)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:235)
	at com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:232)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:50)
	at com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:277)
	at com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:270)
	at com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:50)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:471)
	at com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:690)
	at scala.util.Try$.apply(Try.scala:213)
	at com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:682)
	at com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:523)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:635)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:428)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:371)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:223)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.spark.sql.delta.DeltaOptions
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:419)
	at com.databricks.backend.daemon.driver.ClassLoaders$LibraryClassLoader.loadClass(ClassLoaders.scala:151)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:352)
	... 52 more

```

Also tried to build delta_core directly into the Uber jar without "provided" key word, but it throws another exception
```
Caused by: shade.com.google.common.util.concurrent.ExecutionError: java.lang.VerifyError: Cannot inherit from final class
	at shade.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2047)
	at shade.com.google.common.cache.LocalCache.get(LocalCache.java:3851)
	at shade.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4713)
	at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:464)
	at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:401)
	at org.apache.spark.sql.delta.sources.DeltaSink.<init>(DeltaSink.scala:45)
	at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$2(SplitDeltaLakeSink.scala:42)
	at scala.collection.concurrent.Map.getOrElseUpdate(Map.scala:97)
	at scala.collection.concurrent.Map.getOrElseUpdate$(Map.scala:94)
	at scala.collection.convert.Wrappers$JConcurrentMapWrapper.getOrElseUpdate(Wrappers.scala:333)
	at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$1(SplitDeltaLakeSink.scala:37)
	at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$1$adapted(SplitDeltaLakeSink.scala:33)
	at scala.collection.Iterator.foreach(Iterator.scala:941)
	at scala.collection.Iterator.foreach$(Iterator.scala:941)
	at scala.collection.parallel.immutable.ParHashMap$ParHashMapIterator.foreach(ParHashMap.scala:80)
	at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:974)
	at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
	at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
	at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
	at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:971)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:160)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal$(Tasks.scala:157)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.internal(Tasks.scala:440)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute(Tasks.scala:150)
	at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.compute$(Tasks.scala:149)
	at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.compute(Tasks.scala:440)
	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
	Suppressed: shade.com.google.common.util.concurrent.ExecutionError: java.lang.VerifyError: Cannot inherit from final class
		at shade.com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2047)
		at shade.com.google.common.cache.LocalCache.get(LocalCache.java:3851)
		at shade.com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4713)
		at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:464)
		at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:401)
		at org.apache.spark.sql.delta.sources.DeltaSink.<init>(DeltaSink.scala:45)
		at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$2(SplitDeltaLakeSink.scala:42)
		at scala.collection.concurrent.Map.getOrElseUpdate(Map.scala:97)
		at scala.collection.concurrent.Map.getOrElseUpdate$(Map.scala:94)
		at scala.collection.convert.Wrappers$JConcurrentMapWrapper.getOrElseUpdate(Wrappers.scala:333)
		at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$1(SplitDeltaLakeSink.scala:37)
		at hubble.sink.SplitDeltaLakeSink.$anonfun$writeToDestinations$1$adapted(SplitDeltaLakeSink.scala:33)
		at scala.collection.Iterator.foreach(Iterator.scala:941)
		at scala.collection.Iterator.foreach$(Iterator.scala:941)
		at scala.collection.parallel.immutable.ParHashMap$ParHashMapIterator.foreach(ParHashMap.scala:80)
		at scala.collection.parallel.ParIterableLike$Foreach.leaf(ParIterableLike.scala:974)
		at scala.collection.parallel.Task.$anonfun$tryLeaf$1(Tasks.scala:53)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
		at scala.util.control.Breaks$$anon$1.catchBreak(Breaks.scala:67)
		at scala.collection.parallel.Task.tryLeaf(Tasks.scala:56)
		at scala.collection.parallel.Task.tryLeaf$(Tasks.scala:50)
		at scala.collection.parallel.ParIterableLike$Foreach.tryLeaf(ParIterableLike.scala:971)
		at scala.collection.parallel.AdaptiveWorkStealingTasks$WrappedTask.internal(Tasks.scala:170)
		... 10 more

```

This is what our sbt file look like
```
libraryDependencies ++= Seq(
  "ch.qos.logback" % "logback-classic" % "1.2.3", 
  "com.typesafe.play" %% "play-json" % "2.7.1",
  "io.delta" %% "delta-core" % "1.0.0" % "provided",
  "org.apache.spark" %% "spark-sql" % "3.0.0" % "provided",
  "org.apache.spark" %% "spark-sql-kafka-0-10" % "3.1.1",
  "org.apache.spark" %% "spark-core" % "3.0.0" % "provided",
  "org.apache.hadoop" % "hadoop-aws" % "3.2.0" % "provided",
  "org.apache.hadoop" % "hadoop-client" % "3.2.0" % "provided",
  "org.slf4s" %% "slf4s-api" % "1.7.25",
  "org.scalatest" %% "scalatest" % "3.0.1" % Test,
  "org.scalamock" %% "scalamock" % "4.1.0" % Test
)
```