**Problem**
When running concurrent append and overwrite to delta, the overwrite transaction reads data that is committed after its snapshot read version. This breaks isolation and duplicates data. The metadata (delta log) is handled properly with respect to read version, however the data read from delta is not properly isolated.

**Steps to reproduce**
I've created a test app, which manifests the problem: [delta-concurrency-bug](https://github.com/tomasbartalos/delta-concurrency-bug)

You need to run 2 spark apps:
- **[test.ConcurrentAppend](https://github.com/tomasbartalos/delta-concurrency-bug/blob/master/src/main/scala/test/ConcurrentAppend.scala)** - will continuously append data to delta `/tmp/concurrent_delta/`
- **[test.ConcurrentCompactor](https://github.com/tomasbartalos/delta-concurrency-bug/blob/master/src/main/scala/test/ConcurrentCompactor.scala)** - will reorganize data (overwrite with `dataChange = false`)

**Steps:**

1. start test.ConcurrentAppend, which will continuously append 700 rows to delta, let it run for a while
2. start test.ConcurrentCompactor, which will reorganize data with 20s sleep (so ConcurrentAppend will produce several commits before compaction finishes)
3. when ConcurrentCompactor finishes, kill the ConcurrentAppend app and check the results

**Failed Assertions:**

1. **Delta can't have duplicated data**. Every append inserts 700 records so following must be true:
spark.read.format("delta").load("/tmp/concurrent_delta").groupBy('batchId).count.where('count > 700).collect.isEmpty
res0: Boolean = false

2. **Data inserted by test.ConcurrentCompactor can have as many records as its read version**:
Let's find compactor transaction: 
`grep '"mode":"Overwrite"' /tmp/concurrent_delta/_delta_log/*`
```
{"commitInfo":{"timestamp":1605255834581,"operation":"WRITE","operationParameters":{"mode":"Overwrite","partitionBy":"[\"part\"]","predicate":"part = 0"},"readVersion":410,"isBlindAppend":false,"operationMetrics":{"numFiles":"1","numOutputBytes":"42615","numOutputRows":"289100"}}}
{"add":{"path":"part=0/part-00191-a69fc44e-12aa-46ca-9459-3790b580d94d.c000.snappy.parquet","partitionValues":{"part":"0"},"size":42615,"modificationTime":1605255832000,"dataChange":false}}
{"remove":{"path":"
```
You can see that read version = 410. Now let's time travel to that version in delta and compare counts with AddFile parquet generated by compactor:

```
scala> spark.read.format("delta").option("versionAsOf", 410).load("/tmp/concurrent_delta").count
res2: Long = 287700

scala> spark.read.format("parquet").load("/tmp/concurrent_delta/part=0/part-00191-a69fc44e-12aa-46ca-9459-3790b580d94d.c000.snappy.parquet").count
res4: Long = 289100
```
**As you can see there are 1400 duplicated records (2 * 700 appends)**

This proves that data read by compactor (version 410) is not properly isolated, because there is interference with later commits from ConcurrentAppend transactions. The delta log is handled properly as of version 410 and this is why the extra commits are not included in RemoveFiles of compactor transaction.

